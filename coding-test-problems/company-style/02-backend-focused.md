# ë°±ì—”ë“œ íŠ¹í™” ë¬¸ì œ

## ë¬¸ì œ 1: API ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… (Medium)
**ì‹œê°„ ì œí•œ**: 30ë¶„

í† í° ë²„í‚· ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ì—¬ API ìš”ì²­ì„ ì œí•œí•˜ì„¸ìš”.

```python
import time

class RateLimiter:
    def __init__(self, capacity, refill_rate):
        """
        capacity: ë²„í‚· ìš©ëŸ‰ (ìµœëŒ€ í† í° ìˆ˜)
        refill_rate: ì´ˆë‹¹ í† í° ì¶©ì „ëŸ‰
        """
        pass
    
    def is_allowed(self, tokens_needed=1):
        """
        ìš”ì²­ì´ í—ˆìš©ë˜ëŠ”ì§€ í™•ì¸
        
        ì˜ˆì‹œ:
        limiter = RateLimiter(capacity=10, refill_rate=2)
        
        # ì´ˆê¸°ì—ëŠ” 10ê°œ í† í° ë³´ìœ 
        limiter.is_allowed(5)  # True, 5ê°œ í† í° ì‚¬ìš©, 5ê°œ ë‚¨ìŒ
        limiter.is_allowed(6)  # False, í† í° ë¶€ì¡±
        
        # 1ì´ˆ í›„ 2ê°œ í† í° ì¶©ì „ (ì´ 7ê°œ)
        time.sleep(1)
        limiter.is_allowed(6)  # True, 1ê°œ ë‚¨ìŒ
        """
        pass

# ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
def test_rate_limiter():
    # ë¶„ë‹¹ 60íšŒ ìš”ì²­ ì œí•œ (ì´ˆë‹¹ 1íšŒ)
    limiter = RateLimiter(capacity=60, refill_rate=1)
    
    # ì—°ì† 60íšŒ ìš”ì²­
    for i in range(60):
        assert limiter.is_allowed() == True
    
    # 61ë²ˆì§¸ ìš”ì²­ì€ ê±°ë¶€
    assert limiter.is_allowed() == False
    
    print("Rate limiter test passed!")
```

## ë¬¸ì œ 2: ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ (Medium)
**ì‹œê°„ ì œí•œ**: 35ë¶„

ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ì„ êµ¬í˜„í•˜ì„¸ìš”.

```python
import threading
import queue
import time

class Connection:
    def __init__(self, conn_id):
        self.conn_id = conn_id
        self.in_use = False
        self.created_at = time.time()
    
    def execute_query(self, query):
        # ê°€ìƒì˜ ì¿¼ë¦¬ ì‹¤í–‰
        time.sleep(0.1)  # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        return f"Result for {query} from connection {self.conn_id}"

class ConnectionPool:
    def __init__(self, min_connections=5, max_connections=20, max_idle_time=300):
        """
        min_connections: ìµœì†Œ ì»¤ë„¥ì…˜ ìˆ˜
        max_connections: ìµœëŒ€ ì»¤ë„¥ì…˜ ìˆ˜  
        max_idle_time: ìµœëŒ€ ìœ íœ´ ì‹œê°„ (ì´ˆ)
        """
        pass
    
    def get_connection(self, timeout=10):
        """
        ì»¤ë„¥ì…˜ì„ ê°€ì ¸ì˜¤ê¸°. timeout ì´ˆ ë‚´ì— ì—†ìœ¼ë©´ None ë°˜í™˜
        """
        pass
    
    def return_connection(self, connection):
        """
        ì‚¬ìš© ì™„ë£Œëœ ì»¤ë„¥ì…˜ì„ í’€ì— ë°˜í™˜
        """
        pass
    
    def cleanup_idle_connections(self):
        """
        ìœ íœ´ ì‹œê°„ì´ ì´ˆê³¼ëœ ì»¤ë„¥ì…˜ë“¤ì„ ì •ë¦¬
        """
        pass
    
    def get_stats(self):
        """
        í’€ì˜ í˜„ì¬ ìƒíƒœ ë°˜í™˜
        return: {"total": int, "active": int, "idle": int}
        """
        pass

# ì‚¬ìš© ì˜ˆì‹œ
def database_service_example():
    pool = ConnectionPool(min_connections=2, max_connections=10)
    
    # ë™ì‹œ ìš”ì²­ ì²˜ë¦¬
    def worker():
        conn = pool.get_connection()
        if conn:
            result = conn.execute_query("SELECT * FROM users")
            pool.return_connection(conn)
            return result
        return "No connection available"
    
    # ë©€í‹°ìŠ¤ë ˆë”© í…ŒìŠ¤íŠ¸
    import concurrent.futures
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(worker) for _ in range(10)]
        results = [future.result() for future in futures]
    
    print("Connection pool test completed")
```

## ë¬¸ì œ 3: ë¡œê·¸ ìˆ˜ì§‘ ì‹œìŠ¤í…œ (Hard)
**ì‹œê°„ ì œí•œ**: 45ë¶„

ëŒ€ìš©ëŸ‰ ë¡œê·¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”.

```python
import heapq
from collections import defaultdict, deque
import threading
import time

class LogProcessor:
    def __init__(self, window_size=60):
        """
        window_size: ë¶„ì„ ì‹œê°„ ìœˆë„ìš° (ì´ˆ)
        """
        self.window_size = window_size
        self.logs = deque()
        self.error_count = defaultdict(int)
        self.request_count = 0
        self.response_times = []
        self.lock = threading.Lock()
    
    def add_log(self, log_entry):
        """
        ë¡œê·¸ ì—”íŠ¸ë¦¬ ì¶”ê°€
        
        log_entry = {
            "timestamp": 1640995200.0,
            "level": "ERROR",
            "message": "Database connection failed",
            "service": "user-service",
            "response_time": 250  # milliseconds
        }
        """
        pass
    
    def get_error_rate(self):
        """
        ìµœê·¼ window_size ì´ˆ ë™ì•ˆì˜ ì—ëŸ¬ìœ¨ ë°˜í™˜ (0.0 ~ 1.0)
        """
        pass
    
    def get_average_response_time(self):
        """
        ìµœê·¼ window_size ì´ˆ ë™ì•ˆì˜ í‰ê·  ì‘ë‹µ ì‹œê°„ ë°˜í™˜ (ë°€ë¦¬ì´ˆ)
        """
        pass
    
    def get_top_errors(self, limit=5):
        """
        ìµœê·¼ window_size ì´ˆ ë™ì•ˆ ê°€ì¥ ë§ì´ ë°œìƒí•œ ì—ëŸ¬ Top N
        return: [(error_message, count), ...]
        """
        pass
    
    def cleanup_old_logs(self):
        """
        ìœˆë„ìš° ì‹œê°„ì„ ë²—ì–´ë‚œ ì˜¤ë˜ëœ ë¡œê·¸ ì œê±°
        """
        pass

class AlertManager:
    def __init__(self, log_processor):
        self.log_processor = log_processor
        self.alert_thresholds = {
            "error_rate": 0.05,  # 5% ì´ìƒ
            "avg_response_time": 1000,  # 1ì´ˆ ì´ìƒ
        }
        self.alerts = []
    
    def check_alerts(self):
        """
        ì•Œë¦¼ ì¡°ê±´ì„ ì²´í¬í•˜ê³  í•„ìš”ì‹œ ì•Œë¦¼ ìƒì„±
        """
        pass
    
    def send_alert(self, alert_type, message, severity="WARN"):
        """
        ì•Œë¦¼ ë°œì†¡ (ì‹¤ì œë¡œëŠ” Slack, Email ë“±ìœ¼ë¡œ ë°œì†¡)
        """
        alert = {
            "timestamp": time.time(),
            "type": alert_type,
            "message": message,
            "severity": severity
        }
        self.alerts.append(alert)
        print(f"ğŸš¨ ALERT [{severity}]: {message}")

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
def test_log_processing():
    processor = LogProcessor(window_size=10)  # 10ì´ˆ ìœˆë„ìš°
    alert_manager = AlertManager(processor)
    
    # ìƒ˜í”Œ ë¡œê·¸ ë°ì´í„° ìƒì„±
    import random
    current_time = time.time()
    
    for i in range(100):
        log_entry = {
            "timestamp": current_time - random.uniform(0, 15),
            "level": random.choice(["INFO", "WARN", "ERROR"]),
            "message": f"Sample log message {i}",
            "service": random.choice(["user-service", "order-service", "payment-service"]),
            "response_time": random.randint(50, 2000)
        }
        processor.add_log(log_entry)
    
    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print(f"Error rate: {processor.get_error_rate():.2%}")
    print(f"Average response time: {processor.get_average_response_time():.0f}ms")
    print("Top errors:", processor.get_top_errors(3))
    
    # ì•Œë¦¼ ì²´í¬
    alert_manager.check_alerts()
```

## ë¬¸ì œ 4: ë¶„ì‚° ì‹œìŠ¤í…œ ì¼ê´€ì„± (Hard)
**ì‹œê°„ ì œí•œ**: 50ë¶„

ë¶„ì‚° ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ì¼ê´€ì„±ì„ ë³´ì¥í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•˜ì„¸ìš”.

```python
import hashlib
import threading
from typing import Dict, List, Optional

class ConsistentHash:
    def __init__(self, nodes: List[str], replicas: int = 3):
        """
        ì¼ê´€ì„± í•´ì‹±ì„ êµ¬í˜„í•œ ë¶„ì‚° ìºì‹œ
        nodes: ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ (ì„œë²„ ëª©ë¡)
        replicas: ê°€ìƒ ë…¸ë“œ ìˆ˜ (ë¶€í•˜ ë¶„ì‚°ìš©)
        """
        pass
    
    def _hash(self, key: str) -> int:
        """ë¬¸ìì—´ì„ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜"""
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    def add_node(self, node: str):
        """ìƒˆë¡œìš´ ë…¸ë“œ ì¶”ê°€ (ìŠ¤ì¼€ì¼ ì•„ì›ƒ)"""
        pass
    
    def remove_node(self, node: str):
        """ë…¸ë“œ ì œê±° (ì¥ì•  ìƒí™©)"""
        pass
    
    def get_node(self, key: str) -> str:
        """í‚¤ì— ëŒ€í•œ ë‹´ë‹¹ ë…¸ë“œ ë°˜í™˜"""
        pass

class DistributedCache:
    def __init__(self, nodes: List[str]):
        self.consistent_hash = ConsistentHash(nodes)
        self.local_cache = {}  # ë¡œì»¬ ìºì‹œ ì‹œë®¬ë ˆì´ì…˜
        self.version_vector = {}  # ë²¡í„° í´ë½ìœ¼ë¡œ ë²„ì „ ê´€ë¦¬
        self.lock = threading.Lock()
    
    def get(self, key: str) -> Optional[str]:
        """
        í‚¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê°€ì ¸ì˜´
        ìºì‹œ ë¯¸ìŠ¤ì‹œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ í›„ ìºì‹œì— ì €ì¥
        """
        pass
    
    def put(self, key: str, value: str, ttl: int = 3600):
        """
        í‚¤-ê°’ ìŒì„ ì €ì¥í•˜ê³  ë‹¤ë¥¸ ë…¸ë“œë“¤ì—ê²Œ ì „íŒŒ
        ttl: Time To Live (ì´ˆ)
        """
        pass
    
    def invalidate(self, key: str):
        """
        í‚¤ë¥¼ ë¬´íš¨í™”í•˜ê³  ëª¨ë“  ë…¸ë“œì— ì „íŒŒ
        """
        pass
    
    def replicate(self, key: str, value: str, node: str):
        """
        ë‹¤ë¥¸ ë…¸ë“œë¡œ ë°ì´í„° ë³µì œ
        """
        pass
    
    def handle_node_failure(self, failed_node: str):
        """
        ë…¸ë“œ ì¥ì•  ì²˜ë¦¬ ë° ë°ì´í„° ì¬ë¶„ë°°
        """
        pass

# ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤
def test_distributed_cache():
    # 3ê°œ ë…¸ë“œë¡œ í´ëŸ¬ìŠ¤í„° êµ¬ì„±
    cache = DistributedCache(["node1", "node2", "node3"])
    
    # ë°ì´í„° ì €ì¥
    cache.put("user:1001", '{"name": "Alice", "age": 25}')
    cache.put("user:1002", '{"name": "Bob", "age": 30}')
    
    # ë°ì´í„° ì¡°íšŒ
    user1 = cache.get("user:1001")
    print(f"Retrieved: {user1}")
    
    # ë…¸ë“œ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
    cache.handle_node_failure("node2")
    
    # ì¥ì•  í›„ì—ë„ ë°ì´í„° ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸
    user2 = cache.get("user:1002")
    print(f"After node failure: {user2}")
```

## ğŸ’¡ ë°±ì—”ë“œ ì‹œìŠ¤í…œ ì„¤ê³„ í•µì‹¬ ê°œë…

### 1. í™•ì¥ì„± (Scalability)
- **ìˆ˜í‰ í™•ì¥**: ì„œë²„ ì¶”ê°€ë¡œ ì²˜ë¦¬ëŸ‰ ì¦ê°€
- **ìˆ˜ì§ í™•ì¥**: ì„œë²„ ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ì¦ê°€
- **ì¼ê´€ì„± í•´ì‹±**: ë…¸ë“œ ì¶”ê°€/ì œê±° ì‹œ ë°ì´í„° ì¬ë¶„ë°° ìµœì†Œí™”

### 2. ê°€ìš©ì„± (Availability)
- **ë¶€í•˜ ë¶„ì‚°**: íŠ¸ë˜í”½ì„ ì—¬ëŸ¬ ì„œë²„ì— ë¶„ì‚°
- **ì¥ì•  ë³µêµ¬**: ìë™ ì¥ì•  ê°ì§€ ë° ë³µêµ¬
- **ë°ì´í„° ë³µì œ**: ì¤‘ë³µ ì €ì¥ìœ¼ë¡œ ë°ì´í„° ì†ì‹¤ ë°©ì§€

### 3. ì¼ê´€ì„± (Consistency)
- **ê°•í•œ ì¼ê´€ì„±**: ëª¨ë“  ë…¸ë“œì—ì„œ ë™ì¼í•œ ë°ì´í„°
- **ìµœì¢… ì¼ê´€ì„±**: ì‹œê°„ì´ ì§€ë‚˜ë©´ ì¼ê´€ì„± ë³´ì¥
- **ì¸ê³¼ì  ì¼ê´€ì„±**: ì¸ê³¼ê´€ê³„ê°€ ìˆëŠ” ì—°ì‚° ìˆœì„œ ë³´ì¥

## ğŸ”§ ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œì˜ í™œìš©

```python
# Redis í´ëŸ¬ìŠ¤í„° êµ¬ì„±
class RedisClusterClient:
    def __init__(self, nodes):
        self.consistent_hash = ConsistentHash(nodes)
    
    def set(self, key, value):
        node = self.consistent_hash.get_node(key)
        # ì‹¤ì œë¡œëŠ” Redis ë…¸ë“œì— ì €ì¥
        return f"SET {key}={value} on {node}"

# ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ í†µì‹ 
class ServiceMesh:
    def __init__(self):
        self.rate_limiter = RateLimiter(100, 10)  # ì´ˆë‹¹ 10ê°œ í† í°
    
    def call_service(self, service_name, request):
        if not self.rate_limiter.is_allowed():
            raise Exception("Rate limit exceeded")
        # ì„œë¹„ìŠ¤ í˜¸ì¶œ
        return f"Called {service_name} with {request}"
```

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° êµ¬ì¡° ì‚¬ìš©
from collections import deque
import sys

# í í¬ê¸° ì œí•œ
log_buffer = deque(maxlen=10000)

# ì•½í•œ ì°¸ì¡°ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
import weakref
cache = weakref.WeakValueDictionary()
```

### ë™ì‹œì„± ì²˜ë¦¬
```python
# ìŠ¤ë ˆë“œ ì•ˆì „í•œ ìë£Œêµ¬ì¡°
import queue
import threading

# ìŠ¤ë ˆë“œ í’€ë¡œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
from concurrent.futures import ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=10)
```

## ğŸ¯ ë©´ì ‘ ëŒ€ë¹„ í¬ì¸íŠ¸
- **CAP ì´ë¡ **: ì¼ê´€ì„±, ê°€ìš©ì„±, ë¶„í•  ë‚´ì„± ì¤‘ 2ê°œë§Œ ì„ íƒ ê°€ëŠ¥
- **ACID vs BASE**: ê°•í•œ ì¼ê´€ì„± vs ìµœì¢… ì¼ê´€ì„±
- **ë¡œë“œ ë°¸ëŸ°ì‹± ì•Œê³ ë¦¬ì¦˜**: Round Robin, Weighted, Least Connections
- **ìºì‹± ì „ëµ**: Cache-aside, Write-through, Write-back

ì´ ë¬¸ì œë“¤ì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤ë©´ ì‹œë‹ˆì–´ ë°±ì—”ë“œ ê°œë°œì ìˆ˜ì¤€ì˜ ì—­ëŸ‰ì„ ê°–ì·„ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!