"""
services/kafka_service.py - Apache Kafka ë©”ì‹œì§• ì„œë¹„ìŠ¤

ì´ íŒŒì¼ì—ì„œ ë°°ìš¸ ìˆ˜ ìˆëŠ” ê°œë…ë“¤:
1. Kafka Producer/Consumer íŒ¨í„´
2. ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ (Event-Driven Architecture)
3. ë©”ì‹œì§€ ì§ë ¬í™”/ì—­ì§ë ¬í™”
4. ì—ëŸ¬ í•¸ë“¤ë§ê³¼ ì¬ì‹œë„ ë¡œì§
5. ë¹„ë™ê¸° ë©”ì‹œì§€ ì²˜ë¦¬
6. ë°±í”„ë ˆì…”(Backpressure) ê´€ë¦¬
"""

from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient
from kafka.admin import ConfigResource, ConfigResourceType, NewTopic
from kafka.errors import KafkaError, KafkaTimeoutError
import json
import logging
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime
from config import settings
from models import EventBase, KafkaMessage, EventType
import threading
import asyncio
from concurrent.futures import ThreadPoolExecutor


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KafkaService:
    """
    Kafka ë©”ì‹œì§• ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    
    ì´ë²¤íŠ¸ ë°œí–‰(Producer)ê³¼ êµ¬ë…(Consumer) ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ ë¹„ë™ê¸° í†µì‹ ì˜ í•µì‹¬ ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """
        Kafka ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        """
        self.bootstrap_servers = settings.kafka_bootstrap_servers.split(',')
        self.producer = None
        self.admin_client = None
        self.consumers = {}  # í† í”½ë³„ ì»¨ìŠˆë¨¸ ì €ì¥
        self.executor = ThreadPoolExecutor(max_workers=4)  # ë¹„ë™ê¸° ì‹¤í–‰ìš©
        
        # Producer ì´ˆê¸°í™”
        self._init_producer()
        
        # Admin í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self._init_admin_client()
        
        logger.info("âœ… Kafka ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_producer(self):
        """
        Kafka Producer ì´ˆê¸°í™”
        
        ProducerëŠ” ë©”ì‹œì§€ë¥¼ Kafka í† í”½ìœ¼ë¡œ ì „ì†¡í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
        """
        try:
            self.producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                
                # ë©”ì‹œì§€ ì§ë ¬í™” ì„¤ì •
                value_serializer=lambda v: json.dumps(
                    v, 
                    default=self._json_serializer,
                    ensure_ascii=False
                ).encode('utf-8'),
                key_serializer=lambda k: str(k).encode('utf-8') if k else None,
                
                # ì„±ëŠ¥ ë° ì‹ ë¢°ì„± ì„¤ì •
                acks='all',  # ëª¨ë“  ë³µì œë³¸ì—ì„œ í™•ì¸ë°›ê¸° (ìµœê³  ì‹ ë¢°ì„±)
                retries=3,   # ì‹¤íŒ¨ ì‹œ 3ë²ˆ ì¬ì‹œë„
                max_in_flight_requests_per_connection=1,  # ìˆœì„œ ë³´ì¥
                
                # ë°°ì¹˜ ë° ì••ì¶• ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
                batch_size=16384,  # 16KB ë°°ì¹˜
                linger_ms=10,      # 10ms ëŒ€ê¸° í›„ ì „ì†¡
                compression_type='gzip',  # GZIP ì••ì¶•
                
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                request_timeout_ms=30000,  # 30ì´ˆ
                delivery_timeout_ms=120000,  # 2ë¶„
                
                # ì—ëŸ¬ ì½œë°±
                on_send_error=self._on_send_error
            )
            logger.info("âœ… Kafka Producer ì´ˆê¸°í™” ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"âŒ Kafka Producer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _init_admin_client(self):
        """
        Kafka Admin í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        í† í”½ ê´€ë¦¬, ì„¤ì • ë³€ê²½ ë“±ì˜ ê´€ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        try:
            self.admin_client = KafkaAdminClient(
                bootstrap_servers=self.bootstrap_servers,
                client_id='fastapi_admin',
                request_timeout_ms=30000
            )
            logger.info("âœ… Kafka Admin í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"âŒ Kafka Admin í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.admin_client = None
    
    # Producer ë©”ì„œë“œë“¤
    async def send_event(self, topic: str, event: EventBase, key: Optional[str] = None) -> bool:
        """
        ì´ë²¤íŠ¸ ë°œí–‰
        
        Args:
            topic (str): ëŒ€ìƒ í† í”½
            event (EventBase): ë°œí–‰í•  ì´ë²¤íŠ¸
            key (Optional[str]): ë©”ì‹œì§€ í‚¤ (íŒŒí‹°ì…˜ ê²°ì •ì— ì‚¬ìš©)
            
        Returns:
            bool: ì „ì†¡ ì„±ê³µ ì—¬ë¶€
            
        ë©”ì‹œì§€ í‚¤(key)ëŠ” ê°™ì€ í‚¤ë¥¼ ê°€ì§„ ë©”ì‹œì§€ë“¤ì´ ê°™ì€ íŒŒí‹°ì…˜ìœ¼ë¡œ ê°€ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
        ìˆœì„œê°€ ì¤‘ìš”í•œ ì´ë²¤íŠ¸(ì˜ˆ: ì‚¬ìš©ìë³„ ì´ë²¤íŠ¸)ì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        try:
            # ì´ë²¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            event_data = event.dict() if hasattr(event, 'dict') else event
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            event_data['_metadata'] = {
                'producer': 'fastapi_service',
                'sent_at': datetime.now().isoformat(),
                'topic': topic
            }
            
            # ë¹„ë™ê¸° ì „ì†¡ (ë¸”ë¡œí‚¹í•˜ì§€ ì•ŠìŒ)
            future = self.producer.send(
                topic=topic,
                value=event_data,
                key=key,
                headers=[
                    ('content-type', b'application/json'),
                    ('event-type', event.event_type.encode('utf-8'))
                ]
            )
            
            # ì „ì†¡ ì™„ë£Œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ í¬í•¨)
            record_metadata = future.get(timeout=30)
            
            logger.info(
                f"âœ… ì´ë²¤íŠ¸ ì „ì†¡ ì„±ê³µ: {topic} "
                f"(íŒŒí‹°ì…˜: {record_metadata.partition}, "
                f"ì˜¤í”„ì…‹: {record_metadata.offset})"
            )
            
            return True
            
        except KafkaTimeoutError:
            logger.error(f"âŒ Kafka ì „ì†¡ íƒ€ì„ì•„ì›ƒ: {topic}")
            return False
        except KafkaError as e:
            logger.error(f"âŒ Kafka ì „ì†¡ ì˜¤ë¥˜ ({topic}): {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ ì´ë²¤íŠ¸ ì „ì†¡ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({topic}): {e}")
            return False
    
    async def send_message(self, topic: str, message: Dict[str, Any], key: Optional[str] = None) -> bool:
        """
        ì¼ë°˜ ë©”ì‹œì§€ ì „ì†¡ (ì´ë²¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°)
        """
        try:
            future = self.producer.send(
                topic=topic,
                value=message,
                key=key
            )
            
            record_metadata = future.get(timeout=30)
            
            logger.info(
                f"âœ… ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ: {topic} "
                f"(íŒŒí‹°ì…˜: {record_metadata.partition}, ì˜¤í”„ì…‹: {record_metadata.offset})"
            )
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜ ({topic}): {e}")
            return False
    
    def flush(self, timeout: int = 30):
        """
        Producer ë²„í¼ì— ìˆëŠ” ëª¨ë“  ë©”ì‹œì§€ ì¦‰ì‹œ ì „ì†¡
        
        ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œë‚˜ ì¤‘ìš”í•œ ë©”ì‹œì§€ ì „ì†¡ í›„ í˜¸ì¶œí•©ë‹ˆë‹¤.
        """
        try:
            self.producer.flush(timeout=timeout)
            logger.info("âœ… Kafka Producer flush ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ Kafka Producer flush ì˜¤ë¥˜: {e}")
    
    # Consumer ë©”ì„œë“œë“¤
    def create_consumer(self, 
                       topics: List[str], 
                       group_id: str = None,
                       auto_offset_reset: str = 'latest') -> KafkaConsumer:
        """
        Kafka Consumer ìƒì„±
        
        Args:
            topics (List[str]): êµ¬ë…í•  í† í”½ ëª©ë¡
            group_id (str): ì»¨ìŠˆë¨¸ ê·¸ë£¹ ID
            auto_offset_reset (str): ì˜¤í”„ì…‹ ë¦¬ì…‹ ì •ì±… (earliest/latest)
            
        Returns:
            KafkaConsumer: ìƒì„±ëœ ì»¨ìŠˆë¨¸
            
        ì»¨ìŠˆë¨¸ ê·¸ë£¹:
        - ê°™ì€ ê·¸ë£¹ì˜ ì»¨ìŠˆë¨¸ë“¤ì€ ë©”ì‹œì§€ë¥¼ ë¶„ì‚°í•´ì„œ ì²˜ë¦¬
        - ë‹¤ë¥¸ ê·¸ë£¹ì˜ ì»¨ìŠˆë¨¸ë“¤ì€ ëª¨ë“  ë©”ì‹œì§€ë¥¼ ê°ê° ë°›ìŒ
        """
        try:
            if group_id is None:
                group_id = settings.kafka_group_id
            
            consumer = KafkaConsumer(
                *topics,
                bootstrap_servers=self.bootstrap_servers,
                group_id=group_id,
                
                # ë©”ì‹œì§€ ì—­ì§ë ¬í™”
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                key_deserializer=lambda k: k.decode('utf-8') if k else None,
                
                # ì˜¤í”„ì…‹ ê´€ë¦¬
                auto_offset_reset=auto_offset_reset,
                enable_auto_commit=True,
                auto_commit_interval_ms=5000,  # 5ì´ˆë§ˆë‹¤ ì˜¤í”„ì…‹ ì»¤ë°‹
                
                # ì„¸ì…˜ ê´€ë¦¬
                session_timeout_ms=30000,  # 30ì´ˆ
                heartbeat_interval_ms=3000,  # 3ì´ˆ
                
                # ì„±ëŠ¥ ì„¤ì •
                fetch_max_wait_ms=500,  # ìµœëŒ€ 500ms ëŒ€ê¸°
                max_poll_records=500,   # í•œ ë²ˆì— ìµœëŒ€ 500ê°œ ë©”ì‹œì§€
                
                # ë„¤íŠ¸ì›Œí¬ ì„¤ì •
                request_timeout_ms=40000,  # 40ì´ˆ
                retry_backoff_ms=100
            )
            
            logger.info(f"âœ… Kafka Consumer ìƒì„±: {topics} (ê·¸ë£¹: {group_id})")
            return consumer
            
        except Exception as e:
            logger.error(f"âŒ Kafka Consumer ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def consume_messages(self, 
                              topics: List[str], 
                              message_handler: Callable,
                              group_id: str = None) -> None:
        """
        ë©”ì‹œì§€ ë¹„ë™ê¸° ì†Œë¹„
        
        Args:
            topics (List[str]): êµ¬ë…í•  í† í”½ ëª©ë¡
            message_handler (Callable): ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜
            group_id (str): ì»¨ìŠˆë¨¸ ê·¸ë£¹ ID
            
        ì´ ë©”ì„œë“œëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì‹¤í–‰ë˜ë©° ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        def _consume_loop():
            consumer = self.create_consumer(topics, group_id)
            try:
                logger.info(f"ğŸ”„ ë©”ì‹œì§€ ì†Œë¹„ ì‹œì‘: {topics}")
                
                for message in consumer:
                    try:
                        # ë©”ì‹œì§€ ì •ë³´ ë¡œê¹…
                        logger.info(
                            f"ğŸ“¨ ë©”ì‹œì§€ ìˆ˜ì‹ : {message.topic} "
                            f"(íŒŒí‹°ì…˜: {message.partition}, ì˜¤í”„ì…‹: {message.offset})"
                        )
                        
                        # ë©”ì‹œì§€ ì²˜ë¦¬
                        asyncio.run(message_handler(message))
                        
                    except Exception as e:
                        logger.error(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê³„ì† ì²˜ë¦¬ (ì¥ì•  ê²©ë¦¬)
                        continue
                        
            except KeyboardInterrupt:
                logger.info("â¹ï¸ ë©”ì‹œì§€ ì†Œë¹„ ì¤‘ë‹¨")
            except Exception as e:
                logger.error(f"âŒ ë©”ì‹œì§€ ì†Œë¹„ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                consumer.close()
                logger.info("âœ… Consumer ì—°ê²° ì¢…ë£Œ")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ (ë¹„ë™ê¸° ì²˜ë¦¬)
        future = self.executor.submit(_consume_loop)
        return future
    
    # í† í”½ ê´€ë¦¬
    async def create_topic(self, topic_name: str, num_partitions: int = 3, replication_factor: int = 1) -> bool:
        """
        í† í”½ ìƒì„±
        
        Args:
            topic_name (str): ìƒì„±í•  í† í”½ ì´ë¦„
            num_partitions (int): íŒŒí‹°ì…˜ ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ì¤€ ê²°ì •)
            replication_factor (int): ë³µì œë³¸ ìˆ˜ (ì¥ì•  ëŒ€ì‘)
            
        Returns:
            bool: ìƒì„± ì„±ê³µ ì—¬ë¶€
        """
        if not self.admin_client:
            logger.error("âŒ Admin í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        try:
            topic = NewTopic(
                name=topic_name,
                num_partitions=num_partitions,
                replication_factor=replication_factor,
                topic_configs={
                    'retention.ms': '604800000',  # 7ì¼ê°„ ë³´ê´€
                    'compression.type': 'gzip',
                    'cleanup.policy': 'delete'
                }
            )
            
            # í† í”½ ìƒì„± ìš”ì²­
            future = self.admin_client.create_topics([topic], validate_only=False)
            
            # ê²°ê³¼ ëŒ€ê¸°
            for topic_name, future in future.items():
                future.result()  # ì˜ˆì™¸ ë°œìƒ ì‹œ ì—¬ê¸°ì„œ ìºì¹˜ë¨
            
            logger.info(f"âœ… í† í”½ ìƒì„± ì„±ê³µ: {topic_name}")
            return True
            
        except Exception as e:
            if "already exists" in str(e):
                logger.info(f"â„¹ï¸ í† í”½ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {topic_name}")
                return True
            else:
                logger.error(f"âŒ í† í”½ ìƒì„± ì‹¤íŒ¨ ({topic_name}): {e}")
                return False
    
    async def list_topics(self) -> List[str]:
        """
        í† í”½ ëª©ë¡ ì¡°íšŒ
        """
        if not self.admin_client:
            return []
        
        try:
            metadata = self.admin_client.list_topics()
            topics = list(metadata.topics.keys())
            logger.info(f"ğŸ“‹ í† í”½ ëª©ë¡: {topics}")
            return topics
        except Exception as e:
            logger.error(f"âŒ í† í”½ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def delete_topic(self, topic_name: str) -> bool:
        """
        í† í”½ ì‚­ì œ
        """
        if not self.admin_client:
            return False
        
        try:
            future = self.admin_client.delete_topics([topic_name])
            for topic_name, future in future.items():
                future.result()
            
            logger.info(f"âœ… í† í”½ ì‚­ì œ ì„±ê³µ: {topic_name}")
            return True
        except Exception as e:
            logger.error(f"âŒ í† í”½ ì‚­ì œ ì‹¤íŒ¨ ({topic_name}): {e}")
            return False
    
    # í—¬ìŠ¤ì²´í¬ ë° ëª¨ë‹ˆí„°ë§
    async def health_check(self) -> Dict[str, str]:
        """
        Kafka ì—°ê²° ìƒíƒœ í™•ì¸
        """
        try:
            # Producer ìƒíƒœ í™•ì¸
            if not self.producer:
                return {
                    "status": "unhealthy",
                    "message": "Producerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                    "timestamp": datetime.now().isoformat()
                }
            
            # í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ì¡°íšŒë¡œ ì—°ê²° ìƒíƒœ í™•ì¸
            metadata = self.producer.list_topics(timeout=5)
            
            if metadata:
                broker_count = len(self.producer.cluster.brokers())
                return {
                    "status": "healthy",
                    "message": f"Kafka ì—°ê²° ì •ìƒ (ë¸Œë¡œì»¤ ìˆ˜: {broker_count})",
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "status": "unhealthy",
                    "message": "ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨",
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            return {
                "status": "unhealthy",
                "message": f"Kafka ì˜¤ë¥˜: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _json_serializer(self, obj):
        """JSON ì§ë ¬í™”ìš© ì»¤ìŠ¤í…€ ì‹œë¦¬ì–¼ë¼ì´ì €"""
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif hasattr(obj, 'dict'):  # Pydantic ëª¨ë¸
            return obj.dict()
        elif hasattr(obj, '__dict__'):
            return obj.__dict__
        else:
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
    
    def _on_send_error(self, exception):
        """Producer ì „ì†¡ ì‹¤íŒ¨ ì‹œ í˜¸ì¶œë˜ëŠ” ì½œë°±"""
        logger.error(f"âŒ Kafka Producer ì „ì†¡ ì˜¤ë¥˜: {exception}")
    
    def close(self):
        """
        ëª¨ë“  ì—°ê²° ì¢…ë£Œ
        
        ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ í˜¸ì¶œ
        """
        try:
            if self.producer:
                self.flush()  # ë²„í¼ì— ìˆëŠ” ë©”ì‹œì§€ ì „ì†¡
                self.producer.close()
                logger.info("âœ… Kafka Producer ì—°ê²° ì¢…ë£Œ")
            
            if self.admin_client:
                self.admin_client.close()
                logger.info("âœ… Kafka Admin í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ")
            
            # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            self.executor.shutdown(wait=True)
            
        except Exception as e:
            logger.error(f"âŒ Kafka ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
kafka_service = KafkaService()


# í¸ì˜ í•¨ìˆ˜ë“¤
async def publish_user_event(event_type: EventType, user_id: int, data: Dict[str, Any] = None) -> bool:
    """ì‚¬ìš©ì ì´ë²¤íŠ¸ ë°œí–‰ í¸ì˜ í•¨ìˆ˜"""
    event = EventBase(
        event_type=event_type,
        user_id=user_id,
        data=data or {}
    )
    return await kafka_service.send_event("user_events", event, key=str(user_id))


async def publish_system_event(event_type: str, data: Dict[str, Any]) -> bool:
    """ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ë°œí–‰ í¸ì˜ í•¨ìˆ˜"""
    event = EventBase(
        event_type=event_type,
        data=data
    )
    return await kafka_service.send_event("system_events", event)


if __name__ == "__main__":
    """
    Kafka ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì½”ë“œ
    """
    import asyncio
    
    async def test_kafka_service():
        print("Kafka ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # í† í”½ ìƒì„± í…ŒìŠ¤íŠ¸
        await kafka_service.create_topic("test_topic", num_partitions=2)
        
        # í† í”½ ëª©ë¡ ì¡°íšŒ
        topics = await kafka_service.list_topics()
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í† í”½: {topics}")
        
        # ì´ë²¤íŠ¸ ë°œí–‰ í…ŒìŠ¤íŠ¸
        test_event = EventBase(
            event_type=EventType.USER_CREATED,
            user_id=123,
            data={"email": "test@example.com", "name": "í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì"}
        )
        
        success = await kafka_service.send_event("test_topic", test_event)
        print(f"ì´ë²¤íŠ¸ ë°œí–‰: {success}")
        
        # í—¬ìŠ¤ì²´í¬
        health = await kafka_service.health_check()
        print(f"í—¬ìŠ¤ì²´í¬: {health}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_kafka_service())